
1) We keep ranks for web resources - every web resource is identified by set of shingles. 
This set of shingles is used to match content ( not web page /html code, but text content which is visible to user ) and 
we do have assumption what using similarity matching we can match same content (which looks same and it's same ) published in different
way on different web sites. So every piece of information in web is identified by set of schingles.
2) Every set of schingles have additional part - it's user experience rank, so attributes like 'fake', 'good', 'not sure' and 
amount of user who have this feedback, so can see that this piece of information on internet is treated as 'fake' by this amount
of user who look on this.
3) Challenge here is what we can't keep all schingles for all existing documents, it's just too much, but by real expectation of amount
of users who will install plug in and use it, and also some setting which will limit to use this technology only for unknown / not sure /
resources - like we don't really need to collect and use it for bbc.co.uk, we may trust it whenever, but might be if it post on 
some personal wordpress it might be reasonable. So amount of docs to keep is limited by amount of users and hard setting of what kind of 
web resources are we trying to track.
